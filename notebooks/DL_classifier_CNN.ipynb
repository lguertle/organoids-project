{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamental libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Working libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import timm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, ViTConfig, ResNetForImageClassification\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from importlib import reload\n",
    "import utility\n",
    "reload(utility)\n",
    "from utility import load_data, plot_confusion_matrix, plot_average_f1_scores, train_model, get_classification_details, get_hard_disk_path, show_samples, plot_features_importance, visualize_correlation, get_shap\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Recommendations\n",
    "VGG-16: Despite being relatively deep, VGG-16 can still be effective on smaller datasets, especially when used with transfer learning. Its simplicity and well-understood architecture make it a good starting point.\n",
    "\n",
    "ResNet-50: ResNet models, especially the shallower ones like ResNet-50, are known for their ability to avoid overfitting through the use of residual connections. This can be beneficial for learning from small datasets.\n",
    "\n",
    "MobileNet: Designed for mobile and resource-constrained environments, MobileNets are lightweight and efficient, which can be advantageous when training data is limited.\n",
    "\n",
    "SqueezeNet: This network achieves AlexNet-level accuracy with significantly fewer parameters. Its compact architecture makes it suitable for small datasets and limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE PATHS on local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each folder path represent class (label) :\n",
    "\n",
    "**Folder name - calss name : description**\n",
    "\n",
    "0 - 0 class : dead\n",
    "\n",
    "1 - 1 class : empty\n",
    "\n",
    "2 - 2 class : keep0\n",
    "\n",
    "3 - 3 class : keep1\n",
    "\n",
    "4 - 4 class : keep2\n",
    "\n",
    "5 - 5 class : reseed0\n",
    "\n",
    "6 - 6 class : reseed1\n",
    "\n",
    "7 -  7 class : split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from E:/data_for_DL_augmented/\n"
     ]
    }
   ],
   "source": [
    "# paths for data upload\n",
    "FILE_PATH = get_hard_disk_path(\"DL_augmented\")[:-1] + \"_10/\"\n",
    "TRAIN_FEATURES_PATH_0 = FILE_PATH + 'dead' \n",
    "TRAIN_FEATURES_PATH_1 = FILE_PATH + 'empty'\n",
    "TRAIN_FEATURES_PATH_2 = FILE_PATH + 'keep0'\n",
    "TRAIN_FEATURES_PATH_3 = FILE_PATH + 'keep1'\n",
    "TRAIN_FEATURES_PATH_4 = FILE_PATH + 'keep2'\n",
    "TRAIN_FEATURES_PATH_5 = FILE_PATH + 'reseed0'\n",
    "TRAIN_FEATURES_PATH_6 = FILE_PATH + 'reseed1'\n",
    "TRAIN_FEATURES_PATH_7 = FILE_PATH + 'split'\n",
    "\n",
    "# list with pathe\n",
    "PATHES_LIST = [TRAIN_FEATURES_PATH_0,TRAIN_FEATURES_PATH_1,TRAIN_FEATURES_PATH_2,TRAIN_FEATURES_PATH_3,TRAIN_FEATURES_PATH_4, TRAIN_FEATURES_PATH_5, TRAIN_FEATURES_PATH_6, TRAIN_FEATURES_PATH_7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set : train & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was divided into training and testing sets for each class, with a split of 80% for training and 20% for testing, maintaining the same ratio for each class. However, the dataset exhibits an imbalance issue, with one class having a significantly larger number of samples compared to the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/data_for_DL_augmented_10/dead\n",
      "Class  dead  : train :  117  test :  4\n",
      "E:/data_for_DL_augmented_10/empty\n",
      "Class  empty  : train :  108  test :  3\n",
      "E:/data_for_DL_augmented_10/keep0\n",
      "Class  keep0  : train :  144  test :  5\n",
      "E:/data_for_DL_augmented_10/keep1\n",
      "Class  keep1  : train :  171  test :  5\n",
      "E:/data_for_DL_augmented_10/keep2\n",
      "Class  keep2  : train :  135  test :  4\n",
      "E:/data_for_DL_augmented_10/reseed0\n",
      "Class  reseed0  : train :  36  test :  2\n",
      "E:/data_for_DL_augmented_10/reseed1\n",
      "Class  reseed1  : train :  72  test :  2\n",
      "E:/data_for_DL_augmented_10/split\n",
      "Class  split  : train :  81  test :  3\n"
     ]
    }
   ],
   "source": [
    "labels = ['dead', 'empty', 'keep0', 'keep1', 'keep2', 'reseed0', 'reseed1', 'split']\n",
    "for i, path in enumerate(PATHES_LIST):\n",
    "    print(path)\n",
    "    print(\"Class \", labels[i], \" : train : \",len(os.listdir(os.path.join(path, \"train\"))),\" test : \",len(os.listdir(os.path.join(path, \"test\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_test` is a custom dataset class used for testing or evaluating a machine learning model on a specific set of images. Here's a brief explanation of why it's needed:\n",
    "\n",
    "1. **Path List**: The `path_list` parameter contains the list of paths to directories where the images are located. Each directory represents a different class or category.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for testing. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the test set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for testing. This ensures that the test dataset contains the desired proportion of samples from each class.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the test set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. The image tensor and its corresponding label are then returned as a tuple.\n",
    "\n",
    "By implementing this custom dataset class, you can easily load and process the test data in a standardized manner, making it convenient to evaluate the model's performance on a specific test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)  # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            class_size_test = round(class_size * (1 - ratio))\n",
    "            self.len += class_size_test\n",
    "            self.img_labels.extend([i] * class_size_test)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")][-class_size_test:]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255  # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_train` is another custom dataset class specifically designed for training a machine learning model using augmented images. Here's a brief explanation of why it's needed, along with the rationale behind using data augmentation techniques:\n",
    "\n",
    "1. **Path List**: Similar to the previous class, the `path_list` parameter contains the list of paths to directories where the training images are located.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each training image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for training. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the training set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for training.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the training set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. Additionally, the class applies data augmentation techniques to the training images to enhance the diversity and generalizability of the dataset.\n",
    "\n",
    "8. **Augmentation Techniques**: The `augmentation` method implements several augmentation techniques commonly used for image data. These techniques include random cropping, random horizontal and vertical flipping, and rotation by a random angle between 0 and 45 degrees. By applying these techniques randomly to each training image, the dataset becomes more robust and less sensitive to variations in the input data.\n",
    "\n",
    "The purpose of using data augmentation is to artificially increase the size and variability of the training dataset. This helps prevent overfitting and improves the model's ability to generalize to new, unseen data. By introducing random transformations during training, the model becomes more resilient to variations in the test data and can better handle real-world scenarios.\n",
    "\n",
    "Overall, the `CustomImageDataset_train` class provides a convenient way to load, preprocess, and augment the training data, enabling effective training of machine learning models on a diverse and expanded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_train(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name) # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in sorted(os.listdir(img_dir)) if f.endswith(\".jpg\")])\n",
    "            class_size_train = class_size - round(class_size * (1 - ratio))\n",
    "            self.len += class_size_train\n",
    "            self.img_labels.extend([i] * class_size_train)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")])[:class_size_train])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255. # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = self.augmentation(image) # Applying custom augmentation\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def augmentation(self, image):\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "\n",
    "        # Random rotation by 0, 90, 180, or 270 degrees\n",
    "        degree = random.choice([0, 90, 180, 270])\n",
    "        image = TF.rotate(image, degree)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data to train and test data with ratio 0.8 in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the CustomImageDataset_test and CustomImageDataset_train, it is necessary to specify the desired transformations for each case, such as Normalize and CenterCrop.\n",
    "\n",
    "The following code demonstrates how to define the transformations:\n",
    "\n",
    "For test data transformation using CenterCrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform=None, model_name=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.processor = None\n",
    "        \n",
    "        # Load the processor if a model name is given\n",
    "        if model_name is not None:\n",
    "            self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            # Load all images in the directory\n",
    "            img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            self.img_labels.extend([i] * len(img_files))\n",
    "            self.img_sort.extend([os.path.join(img_dir, f) for f in img_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_sort)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float()\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if image.shape[0] == 1:  # assuming (channel, height, width)\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model if a processor is available\n",
    "        if self.processor:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path_list = [os.path.normpath(os.path.join(path,\"train\")) for path in PATHES_LIST]\n",
    "test_path_list = [os.path.normpath(os.path.join(path,\"test\")) for path in PATHES_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\data_for_DL_augmented_10\\\\dead\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\empty\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\keep0\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\keep1\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\keep2\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\reseed0\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\reseed1\\\\train', 'E:\\\\data_for_DL_augmented_10\\\\split\\\\train']\n"
     ]
    }
   ],
   "source": [
    "print(train_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f0c3bd936f4ffba4841e71034e9e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomImageDataset_test(path_list=test_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name=\"microsoft/resnet-50\")\n",
    "\n",
    "train_dataset = CustomImageDataset_test(path_list=train_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name=\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 864\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset), len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d29eb3c86f422190d7bf275e39437a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/103M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/resnet-50\"\n",
    "num_labels = len(labels)  # The number of unique labels/classes in your dataset\n",
    "\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8yhuxed5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-shadow-43</strong> at: <a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/8yhuxed5' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification/runs/8yhuxed5</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231115_161758-8yhuxed5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8yhuxed5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab0191239164ad98bd6089f1880dc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lguertle\\organoids-project\\notebooks\\wandb\\run-20231115_162020-zirrzhpw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/zirrzhpw' target=\"_blank\">silver-paper-44</a></strong> to <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/zirrzhpw' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification/runs/zirrzhpw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[677, 361, 111, 115, 677, 780, 885, 677, 310, 700, 301, 792, 419, 174, 37, 971, 844, 111, 488, 488, 254, 183, 183, 350, 174, 174, 111, 164]\n",
      "Number of classes in all_labels_list: 8\n",
      "Number of F1 scores: 28\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lguertle\\organoids-project\\notebooks\\DL_classifier_CNN.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of classes in all_labels_list: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(all_labels_list)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of F1 scores: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(f1_scores)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m f1_scores_dict \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mall_labels_list[idx]\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m: f1 \u001b[39mfor\u001b[39;49;00m idx, f1 \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(f1_scores)}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m epoch_f1_scores_dict_list\u001b[39m.\u001b[39mappend(f1_scores_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# It might be useful to also get the classification report for all metrics\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lguertle\\organoids-project\\notebooks\\DL_classifier_CNN.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of classes in all_labels_list: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(all_labels_list)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of F1 scores: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(f1_scores)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m f1_scores_dict \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mall_labels_list[idx]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: f1 \u001b[39mfor\u001b[39;00m idx, f1 \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(f1_scores)}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m epoch_f1_scores_dict_list\u001b[39m.\u001b[39mappend(f1_scores_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lguertle/organoids-project/notebooks/DL_classifier_CNN.ipynb#X34sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# It might be useful to also get the classification report for all metrics\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "all_labels_list = ['dead', 'empty', 'keep0', 'keep1', 'keep2', 'reseed0', 'reseed1', 'split']\n",
    "\n",
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"organoid_classification\", entity=\"laurent-gurtler\")\n",
    "\n",
    "# Configurations (hyperparameters and model architecture)\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.00001\n",
    "config.batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "epoch_f1_scores_dict_list = []\n",
    "\n",
    "num_epochs = 20  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).logits\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Logging the training loss\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": running_loss/len(train_loader)})\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Move the labels and predictions to CPU for sklearn metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = 100 * correct / total\n",
    "    f1_scores = f1_score(all_labels, all_predictions, average=None, zero_division=0)  # This will give you an array of F1 scores per class\n",
    "    print(all_predictions)\n",
    "    print(f\"Number of classes in all_labels_list: {len(all_labels_list)}\")\n",
    "    print(f\"Number of F1 scores: {len(f1_scores)}\")\n",
    "    f1_scores_dict = {f\"{all_labels_list[idx]}\": f1 for idx, f1 in enumerate(f1_scores)}\n",
    "    epoch_f1_scores_dict_list.append(f1_scores_dict)\n",
    "\n",
    "    # It might be useful to also get the classification report for all metrics\n",
    "    classification_rep = classification_report(all_labels, all_predictions, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Logging the individual F1 scores\n",
    "    for idx, f1 in enumerate(f1_scores):\n",
    "        wandb.log({f\"class_{idx}_f1_score\": f1})\n",
    "\n",
    "    # Logging the classification report\n",
    "    wandb.log({\"classification_report\": classification_rep})\n",
    "\n",
    "    wandb.log({\"epoch\": epoch, \"validation_loss\": validation_loss/len(test_loader)})\n",
    "\n",
    "    # Print statistics for each class\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, \"\n",
    "        f\"Validation Loss: {validation_loss/len(test_loader)}, Accuracy: {accuracy} %\")\n",
    "    for idx, f1 in enumerate(f1_scores):\n",
    "        print(f\"Class {idx} F1 Score: {f1}\")\n",
    "\n",
    "if len(epoch_f1_scores_dict_list) >= 5:\n",
    "    # Get the last 5 epochs' F1-scores for each class\n",
    "    last_5_epochs_f1_scores = epoch_f1_scores_dict_list[-5:]\n",
    "\n",
    "    # Initialize a dictionary to hold the cumulative F1-scores for the last 5 epochs\n",
    "    cumulative_f1_scores = {label: 0 for label in all_labels_list}\n",
    "\n",
    "    # Sum the F1-scores for each label across the last 5 epochs\n",
    "    for epoch_dict in last_5_epochs_f1_scores:\n",
    "        for label, score in epoch_dict.items():\n",
    "            cumulative_f1_scores[label] += score\n",
    "\n",
    "    # Calculate the average F1-scores for each label\n",
    "    average_f1_scores = {label: score / 5 for label, score in cumulative_f1_scores.items()}\n",
    "\n",
    "    plot_average_f1_scores(all_labels_list, average_f1_scores)\n",
    "\n",
    "\n",
    "# After the loop, you may want to log the overall performance\n",
    "wandb.log({\n",
    "    \"final_accuracy\": accuracy,\n",
    "    \"final_f1_scores\": f1_scores.tolist(),  # Convert to list if necessary\n",
    "    \"final_classification_report\": classification_rep\n",
    "})\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Close the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
