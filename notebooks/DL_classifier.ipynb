{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fundamental libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Working libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, ViTConfig\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from importlib import reload\n",
    "import utility\n",
    "reload(utility)\n",
    "from utility import load_data, plot_confusion_matrix, plot_average_f1_scores, train_model, get_classification_details, get_hard_disk_path, show_samples, plot_features_importance, visualize_correlation, get_shap\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE PATHS on local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each folder path represent class (label) :\n",
    "\n",
    "**Folder name - calss name : description**\n",
    "\n",
    "0 - 0 class : dead\n",
    "\n",
    "1 - 1 class : empty\n",
    "\n",
    "2 - 2 class : keep0\n",
    "\n",
    "3 - 3 class : keep1\n",
    "\n",
    "4 - 4 class : keep2\n",
    "\n",
    "5 - 5 class : reseed0\n",
    "\n",
    "6 - 6 class : reseed1\n",
    "\n",
    "7 -  7 class : split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from D:/data_for_DL_augmented/\n"
     ]
    }
   ],
   "source": [
    "# paths for data upload\n",
    "FILE_PATH = get_hard_disk_path(\"DL\")\n",
    "TRAIN_FEATURES_PATH_0 = FILE_PATH + 'dead' \n",
    "TRAIN_FEATURES_PATH_1 = FILE_PATH + 'empty'\n",
    "TRAIN_FEATURES_PATH_2 = FILE_PATH + 'keep0'\n",
    "TRAIN_FEATURES_PATH_3 = FILE_PATH + 'keep1'\n",
    "TRAIN_FEATURES_PATH_4 = FILE_PATH + 'keep2'\n",
    "TRAIN_FEATURES_PATH_5 = FILE_PATH + 'reseed0'\n",
    "TRAIN_FEATURES_PATH_6 = FILE_PATH + 'reseed1'\n",
    "TRAIN_FEATURES_PATH_7 = FILE_PATH + 'split'\n",
    "\n",
    "# list with pathe\n",
    "PATHES_LIST = [TRAIN_FEATURES_PATH_0,TRAIN_FEATURES_PATH_1,TRAIN_FEATURES_PATH_2,TRAIN_FEATURES_PATH_3,TRAIN_FEATURES_PATH_4, TRAIN_FEATURES_PATH_5, TRAIN_FEATURES_PATH_6, TRAIN_FEATURES_PATH_7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set : train & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was divided into training and testing sets for each class, with a split of 80% for training and 20% for testing, maintaining the same ratio for each class. However, the dataset exhibits an imbalance issue, with one class having a significantly larger number of samples compared to the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/data_for_DL_augmented/dead\n",
      "Class  dead  : train :  78  test :  4\n",
      "D:/data_for_DL_augmented/empty\n",
      "Class  empty  : train :  72  test :  3\n",
      "D:/data_for_DL_augmented/keep0\n",
      "Class  keep0  : train :  96  test :  5\n",
      "D:/data_for_DL_augmented/keep1\n",
      "Class  keep1  : train :  114  test :  5\n",
      "D:/data_for_DL_augmented/keep2\n",
      "Class  keep2  : train :  90  test :  4\n",
      "D:/data_for_DL_augmented/reseed0\n",
      "Class  reseed0  : train :  24  test :  2\n",
      "D:/data_for_DL_augmented/reseed1\n",
      "Class  reseed1  : train :  48  test :  2\n",
      "D:/data_for_DL_augmented/split\n",
      "Class  split  : train :  54  test :  3\n"
     ]
    }
   ],
   "source": [
    "labels = ['dead', 'empty', 'keep0', 'keep1', 'keep2', 'reseed0', 'reseed1', 'split']\n",
    "for i, path in enumerate(PATHES_LIST):\n",
    "    print(path)\n",
    "    print(\"Class \", labels[i], \" : train : \",len(os.listdir(os.path.join(path, \"train\"))),\" test : \",len(os.listdir(os.path.join(path, \"test\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_test` is a custom dataset class used for testing or evaluating a machine learning model on a specific set of images. Here's a brief explanation of why it's needed:\n",
    "\n",
    "1. **Path List**: The `path_list` parameter contains the list of paths to directories where the images are located. Each directory represents a different class or category.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for testing. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the test set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for testing. This ensures that the test dataset contains the desired proportion of samples from each class.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the test set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. The image tensor and its corresponding label are then returned as a tuple.\n",
    "\n",
    "By implementing this custom dataset class, you can easily load and process the test data in a standardized manner, making it convenient to evaluate the model's performance on a specific test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)  # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            class_size_test = round(class_size * (1 - ratio))\n",
    "            self.len += class_size_test\n",
    "            self.img_labels.extend([i] * class_size_test)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")][-class_size_test:]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255  # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_train` is another custom dataset class specifically designed for training a machine learning model using augmented images. Here's a brief explanation of why it's needed, along with the rationale behind using data augmentation techniques:\n",
    "\n",
    "1. **Path List**: Similar to the previous class, the `path_list` parameter contains the list of paths to directories where the training images are located.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each training image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for training. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the training set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for training.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the training set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. Additionally, the class applies data augmentation techniques to the training images to enhance the diversity and generalizability of the dataset.\n",
    "\n",
    "8. **Augmentation Techniques**: The `augmentation` method implements several augmentation techniques commonly used for image data. These techniques include random cropping, random horizontal and vertical flipping, and rotation by a random angle between 0 and 45 degrees. By applying these techniques randomly to each training image, the dataset becomes more robust and less sensitive to variations in the input data.\n",
    "\n",
    "The purpose of using data augmentation is to artificially increase the size and variability of the training dataset. This helps prevent overfitting and improves the model's ability to generalize to new, unseen data. By introducing random transformations during training, the model becomes more resilient to variations in the test data and can better handle real-world scenarios.\n",
    "\n",
    "Overall, the `CustomImageDataset_train` class provides a convenient way to load, preprocess, and augment the training data, enabling effective training of machine learning models on a diverse and expanded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset_train(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name) # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in sorted(os.listdir(img_dir)) if f.endswith(\".jpg\")])\n",
    "            class_size_train = class_size - round(class_size * (1 - ratio))\n",
    "            self.len += class_size_train\n",
    "            self.img_labels.extend([i] * class_size_train)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")])[:class_size_train])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255. # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = self.augmentation(image) # Applying custom augmentation\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def augmentation(self, image):\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "\n",
    "        # Random rotation by 0, 90, 180, or 270 degrees\n",
    "        degree = random.choice([0, 90, 180, 270])\n",
    "        image = TF.rotate(image, degree)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data to train and test data with ratio 0.8 in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the CustomImageDataset_test and CustomImageDataset_train, it is necessary to specify the desired transformations for each case, such as Normalize and CenterCrop.\n",
    "\n",
    "The following code demonstrates how to define the transformations:\n",
    "\n",
    "For test data transformation using CenterCrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform=None, model_name=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.processor = None\n",
    "        \n",
    "        # Load the processor if a model name is given\n",
    "        if model_name is not None:\n",
    "            self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            # Load all images in the directory\n",
    "            img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            self.img_labels.extend([i] * len(img_files))\n",
    "            self.img_sort.extend([os.path.join(img_dir, f) for f in img_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_sort)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float()\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if image.shape[0] == 1:  # assuming (channel, height, width)\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model if a processor is available\n",
    "        if self.processor:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_list = [os.path.normpath(os.path.join(path,\"train\")) for path in PATHES_LIST]\n",
    "test_path_list = [os.path.normpath(os.path.join(path,\"test\")) for path in PATHES_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\data_for_DL_augmented\\\\dead\\\\train', 'D:\\\\data_for_DL_augmented\\\\empty\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep0\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep1\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep2\\\\train', 'D:\\\\data_for_DL_augmented\\\\reseed0\\\\train', 'D:\\\\data_for_DL_augmented\\\\reseed1\\\\train', 'D:\\\\data_for_DL_augmented\\\\split\\\\train']\n"
     ]
    }
   ],
   "source": [
    "print(train_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset_test(path_list=test_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')\n",
    "\n",
    "train_dataset = CustomImageDataset_test(path_list=train_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 576\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset), len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "num_labels = len(labels)  # The number of unique labels/classes in your dataset\n",
    "\n",
    "# Load the configuration of the model\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Instantiate the model with the new configuration\n",
    "model = ViTForImageClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c578df7b44e4baf2017a1b3748c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777932999, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: c:\\Users\\Laurent\\anaconda3\\envs\\organoids-project\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py 852 getcaller\n"
     ]
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Laurent\\Documents\\organoids-project\\notebooks\\DL_classifier.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Laurent/Documents/organoids-project/notebooks/DL_classifier.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize a new wandb run\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Laurent/Documents/organoids-project/notebooks/DL_classifier.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39morganoid_classification\u001b[39;49m\u001b[39m\"\u001b[39;49m, entity\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlaurent-gurtler\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Laurent/Documents/organoids-project/notebooks/DL_classifier.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Configurations (hyperparameters and model architecture)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Laurent/Documents/organoids-project/notebooks/DL_classifier.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[1;32mc:\\Users\\Laurent\\anaconda3\\envs\\organoids-project\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1189\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \u001b[39mif\u001b[39;00m logger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1188\u001b[0m         logger\u001b[39m.\u001b[39mexception(\u001b[39mstr\u001b[39m(e))\n\u001b[1;32m-> 1189\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1190\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1191\u001b[0m     \u001b[39massert\u001b[39;00m logger\n",
      "File \u001b[1;32mc:\\Users\\Laurent\\anaconda3\\envs\\organoids-project\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1170\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1168\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1170\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m   1171\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[0;32m   1172\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Laurent\\anaconda3\\envs\\organoids-project\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:785\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m         backend\u001b[39m.\u001b[39mcleanup()\n\u001b[0;32m    784\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteardown()\n\u001b[1;32m--> 785\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[0;32m    787\u001b[0m \u001b[39massert\u001b[39;00m run_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m run_result\u001b[39m.\u001b[39mHasField(\u001b[39m\"\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-"
     ]
    }
   ],
   "source": [
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"organoid_classification\", entity=\"laurent-gurtler\")\n",
    "\n",
    "# Configurations (hyperparameters and model architecture)\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "num_epochs = 100  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images).logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Logging the loss\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": running_loss/len(train_loader)})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Logging the accuracy\n",
    "    wandb.log({\"epoch\": epoch, \"accuracy\": accuracy})\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy} %\")\n",
    "\n",
    "# Final log\n",
    "wandb.log({\"final_accuracy\": accuracy})\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Close the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
