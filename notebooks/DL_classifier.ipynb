{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamental libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Working libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, ViTConfig\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from importlib import reload\n",
    "import utility\n",
    "reload(utility)\n",
    "from utility import load_data, plot_confusion_matrix, plot_average_f1_scores, train_model, get_classification_details, get_hard_disk_path, show_samples, plot_features_importance, visualize_correlation, get_shap\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE PATHS on local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each folder path represent class (label) :\n",
    "\n",
    "**Folder name - calss name : description**\n",
    "\n",
    "0 - 0 class : dead\n",
    "\n",
    "1 - 1 class : empty\n",
    "\n",
    "2 - 2 class : keep0\n",
    "\n",
    "3 - 3 class : keep1\n",
    "\n",
    "4 - 4 class : keep2\n",
    "\n",
    "5 - 5 class : reseed0\n",
    "\n",
    "6 - 6 class : reseed1\n",
    "\n",
    "7 -  7 class : split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from E:/data_for_DL_augmented/\n"
     ]
    }
   ],
   "source": [
    "# paths for data upload\n",
    "FILE_PATH = get_hard_disk_path(\"DL\")\n",
    "TRAIN_FEATURES_PATH_0 = FILE_PATH + 'dead' \n",
    "TRAIN_FEATURES_PATH_1 = FILE_PATH + 'empty'\n",
    "TRAIN_FEATURES_PATH_2 = FILE_PATH + 'keep0'\n",
    "TRAIN_FEATURES_PATH_3 = FILE_PATH + 'keep1'\n",
    "TRAIN_FEATURES_PATH_4 = FILE_PATH + 'keep2'\n",
    "TRAIN_FEATURES_PATH_5 = FILE_PATH + 'reseed0'\n",
    "TRAIN_FEATURES_PATH_6 = FILE_PATH + 'reseed1'\n",
    "TRAIN_FEATURES_PATH_7 = FILE_PATH + 'split'\n",
    "\n",
    "# list with pathe\n",
    "PATHES_LIST = [TRAIN_FEATURES_PATH_0,TRAIN_FEATURES_PATH_1,TRAIN_FEATURES_PATH_2,TRAIN_FEATURES_PATH_3,TRAIN_FEATURES_PATH_4, TRAIN_FEATURES_PATH_5, TRAIN_FEATURES_PATH_6, TRAIN_FEATURES_PATH_7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set : train & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was divided into training and testing sets for each class, with a split of 80% for training and 20% for testing, maintaining the same ratio for each class. However, the dataset exhibits an imbalance issue, with one class having a significantly larger number of samples compared to the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/data_for_DL_augmented/dead\n",
      "Class  dead  : train :  78  test :  4\n",
      "E:/data_for_DL_augmented/empty\n",
      "Class  empty  : train :  72  test :  3\n",
      "E:/data_for_DL_augmented/keep0\n",
      "Class  keep0  : train :  96  test :  5\n",
      "E:/data_for_DL_augmented/keep1\n",
      "Class  keep1  : train :  114  test :  5\n",
      "E:/data_for_DL_augmented/keep2\n",
      "Class  keep2  : train :  90  test :  4\n",
      "E:/data_for_DL_augmented/reseed0\n",
      "Class  reseed0  : train :  24  test :  2\n",
      "E:/data_for_DL_augmented/reseed1\n",
      "Class  reseed1  : train :  48  test :  2\n",
      "E:/data_for_DL_augmented/split\n",
      "Class  split  : train :  54  test :  3\n"
     ]
    }
   ],
   "source": [
    "labels = ['dead', 'empty', 'keep0', 'keep1', 'keep2', 'reseed0', 'reseed1', 'split']\n",
    "for i, path in enumerate(PATHES_LIST):\n",
    "    print(path)\n",
    "    print(\"Class \", labels[i], \" : train : \",len(os.listdir(os.path.join(path, \"train\"))),\" test : \",len(os.listdir(os.path.join(path, \"test\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_test` is a custom dataset class used for testing or evaluating a machine learning model on a specific set of images. Here's a brief explanation of why it's needed:\n",
    "\n",
    "1. **Path List**: The `path_list` parameter contains the list of paths to directories where the images are located. Each directory represents a different class or category.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for testing. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the test set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for testing. This ensures that the test dataset contains the desired proportion of samples from each class.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the test set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. The image tensor and its corresponding label are then returned as a tuple.\n",
    "\n",
    "By implementing this custom dataset class, you can easily load and process the test data in a standardized manner, making it convenient to evaluate the model's performance on a specific test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)  # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            class_size_test = round(class_size * (1 - ratio))\n",
    "            self.len += class_size_test\n",
    "            self.img_labels.extend([i] * class_size_test)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")][-class_size_test:]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255  # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_train` is another custom dataset class specifically designed for training a machine learning model using augmented images. Here's a brief explanation of why it's needed, along with the rationale behind using data augmentation techniques:\n",
    "\n",
    "1. **Path List**: Similar to the previous class, the `path_list` parameter contains the list of paths to directories where the training images are located.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each training image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for training. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the training set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for training.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the training set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. Additionally, the class applies data augmentation techniques to the training images to enhance the diversity and generalizability of the dataset.\n",
    "\n",
    "8. **Augmentation Techniques**: The `augmentation` method implements several augmentation techniques commonly used for image data. These techniques include random cropping, random horizontal and vertical flipping, and rotation by a random angle between 0 and 45 degrees. By applying these techniques randomly to each training image, the dataset becomes more robust and less sensitive to variations in the input data.\n",
    "\n",
    "The purpose of using data augmentation is to artificially increase the size and variability of the training dataset. This helps prevent overfitting and improves the model's ability to generalize to new, unseen data. By introducing random transformations during training, the model becomes more resilient to variations in the test data and can better handle real-world scenarios.\n",
    "\n",
    "Overall, the `CustomImageDataset_train` class provides a convenient way to load, preprocess, and augment the training data, enabling effective training of machine learning models on a diverse and expanded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_train(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name) # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in sorted(os.listdir(img_dir)) if f.endswith(\".jpg\")])\n",
    "            class_size_train = class_size - round(class_size * (1 - ratio))\n",
    "            self.len += class_size_train\n",
    "            self.img_labels.extend([i] * class_size_train)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")])[:class_size_train])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255. # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = self.augmentation(image) # Applying custom augmentation\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def augmentation(self, image):\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "\n",
    "        # Random rotation by 0, 90, 180, or 270 degrees\n",
    "        degree = random.choice([0, 90, 180, 270])\n",
    "        image = TF.rotate(image, degree)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data to train and test data with ratio 0.8 in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the CustomImageDataset_test and CustomImageDataset_train, it is necessary to specify the desired transformations for each case, such as Normalize and CenterCrop.\n",
    "\n",
    "The following code demonstrates how to define the transformations:\n",
    "\n",
    "For test data transformation using CenterCrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform=None, model_name=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.processor = None\n",
    "        \n",
    "        # Load the processor if a model name is given\n",
    "        if model_name is not None:\n",
    "            self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            # Load all images in the directory\n",
    "            img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            self.img_labels.extend([i] * len(img_files))\n",
    "            self.img_sort.extend([os.path.join(img_dir, f) for f in img_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_sort)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float()\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if image.shape[0] == 1:  # assuming (channel, height, width)\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model if a processor is available\n",
    "        if self.processor:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path_list = [os.path.normpath(os.path.join(path,\"train\")) for path in PATHES_LIST]\n",
    "test_path_list = [os.path.normpath(os.path.join(path,\"test\")) for path in PATHES_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\data_for_DL_augmented\\\\dead\\\\train', 'E:\\\\data_for_DL_augmented\\\\empty\\\\train', 'E:\\\\data_for_DL_augmented\\\\keep0\\\\train', 'E:\\\\data_for_DL_augmented\\\\keep1\\\\train', 'E:\\\\data_for_DL_augmented\\\\keep2\\\\train', 'E:\\\\data_for_DL_augmented\\\\reseed0\\\\train', 'E:\\\\data_for_DL_augmented\\\\reseed1\\\\train', 'E:\\\\data_for_DL_augmented\\\\split\\\\train']\n"
     ]
    }
   ],
   "source": [
    "print(train_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset_test(path_list=test_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')\n",
    "\n",
    "train_dataset = CustomImageDataset_test(path_list=train_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 576\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset), len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "num_labels = len(labels)  # The number of unique labels/classes in your dataset\n",
    "\n",
    "# Load the configuration of the model\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Instantiate the model with the new configuration\n",
    "model = ViTForImageClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForImageClassificationWithAttention(ViTForImageClassification):\n",
    "    \"\"\"\n",
    "    This class extends ViTForImageClassification to return attention weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, pixel_values, output_attentions=True):\n",
    "        outputs = self.vit(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        attention_weights = outputs.attentions if output_attentions else None\n",
    "        return logits, attention_weights\n",
    "\n",
    "# Replace the original ViT model with the modified one that returns attention weights\n",
    "model = ViTForImageClassificationWithAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\lguertle\\organoids-project\\notebooks\\wandb\\run-20231107_163922-dxi5601m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/dxi5601m' target=\"_blank\">toasty-hill-20</a></strong> to <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/dxi5601m' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification/runs/dxi5601m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.078055739402771, Validation Loss: 1.9008293151855469, Accuracy: 25.0 %\n",
      "Class 0 F1 Score: 0.0\n",
      "Class 1 F1 Score: 0.6666666666666666\n",
      "Class 2 F1 Score: 0.0\n",
      "Class 3 F1 Score: 0.33333333333333337\n",
      "Class 4 F1 Score: 0.0\n",
      "Class 5 F1 Score: 0.0\n",
      "Class 6 F1 Score: 0.0\n",
      "Class 7 F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"organoid_classification\", entity=\"laurent-gurtler\")\n",
    "\n",
    "# Configurations (hyperparameters and model architecture)\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Logging the training loss\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": running_loss/len(train_loader)})\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, attention_weights = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Move the labels and predictions to CPU for sklearn metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            last_layer_attention = attention_weights[-1]\n",
    "\n",
    "            if epoch == num_epochs - 1:  # Example condition to visualize on the last epoch\n",
    "                # Choose one sample to visualize, e.g., the first in the batch\n",
    "                sample_attention_weights = attention_weights[-1][0]  # Last layer, first in batch\n",
    "                sample_attention_map_cls = sample_attention_weights.mean(0)[1:]  # Ignore [CLS] token\n",
    "\n",
    "                # Resize from (num_patches,) to (H, W) where H*W = num_patches\n",
    "                attention_map_resized = cv2.resize(sample_attention_map_cls.cpu().numpy(), (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                # Overlay the attention map with the original image\n",
    "                # Assuming the first image in the batch is transformed to have pixel values [0,1]\n",
    "                original_image = images[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                plt.imshow(original_image)\n",
    "                plt.imshow(attention_map_resized, cmap='jet', alpha=0.5)\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "\n",
    "        # Now you might want to average across the heads, and ignore the class token:\n",
    "        # Note: last_layer_attention[:, :, 0, 1:] has shape [32, num_heads, num_patches]\n",
    "        attention_map = last_layer_attention.mean(dim=1)[:, 0, 1:].detach().cpu()\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = 100 * correct / total\n",
    "    f1_scores = f1_score(all_labels, all_predictions, average=None)  # This will give you an array of F1 scores per class\n",
    "\n",
    "    # It might be useful to also get the classification report for all metrics\n",
    "    classification_rep = classification_report(all_labels, all_predictions, output_dict=True)\n",
    "\n",
    "    # Logging the individual F1 scores\n",
    "    for idx, f1 in enumerate(f1_scores):\n",
    "        wandb.log({f\"class_{idx}_f1_score\": f1})\n",
    "\n",
    "    # Logging the classification report\n",
    "    wandb.log({\"classification_report\": classification_rep})\n",
    "\n",
    "    # Print statistics for each class\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, \"\n",
    "        f\"Validation Loss: {validation_loss/len(test_loader)}, Accuracy: {accuracy} %\")\n",
    "    for idx, f1 in enumerate(f1_scores):\n",
    "        print(f\"Class {idx} F1 Score: {f1}\")\n",
    "\n",
    "# After the loop, you may want to log the overall performance\n",
    "wandb.log({\n",
    "    \"final_accuracy\": accuracy,\n",
    "    \"final_f1_scores\": f1_scores.tolist(),  # Convert to list if necessary\n",
    "    \"final_classification_report\": classification_rep\n",
    "})\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Close the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
