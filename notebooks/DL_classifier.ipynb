{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fundamental libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Working libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, ViTConfig\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import f1_score\n",
    "from importlib import reload\n",
    "import utility\n",
    "reload(utility)\n",
    "from utility import load_data, plot_confusion_matrix, plot_average_f1_scores, train_model, get_classification_details, get_hard_disk_path, show_samples, plot_features_importance, visualize_correlation, get_shap\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE PATHS on local environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each folder path represent class (label) :\n",
    "\n",
    "**Folder name - calss name : description**\n",
    "\n",
    "0 - 0 class : dead\n",
    "\n",
    "1 - 1 class : empty\n",
    "\n",
    "2 - 2 class : keep0\n",
    "\n",
    "3 - 3 class : keep1\n",
    "\n",
    "4 - 4 class : keep2\n",
    "\n",
    "5 - 5 class : reseed0\n",
    "\n",
    "6 - 6 class : reseed1\n",
    "\n",
    "7 -  7 class : split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from D:/data_for_DL_augmented/\n"
     ]
    }
   ],
   "source": [
    "# paths for data upload\n",
    "FILE_PATH = get_hard_disk_path(\"DL\")\n",
    "TRAIN_FEATURES_PATH_0 = FILE_PATH + 'dead' \n",
    "TRAIN_FEATURES_PATH_1 = FILE_PATH + 'empty'\n",
    "TRAIN_FEATURES_PATH_2 = FILE_PATH + 'keep0'\n",
    "TRAIN_FEATURES_PATH_3 = FILE_PATH + 'keep1'\n",
    "TRAIN_FEATURES_PATH_4 = FILE_PATH + 'keep2'\n",
    "TRAIN_FEATURES_PATH_5 = FILE_PATH + 'reseed0'\n",
    "TRAIN_FEATURES_PATH_6 = FILE_PATH + 'reseed1'\n",
    "TRAIN_FEATURES_PATH_7 = FILE_PATH + 'split'\n",
    "\n",
    "# list with pathe\n",
    "PATHES_LIST = [TRAIN_FEATURES_PATH_0,TRAIN_FEATURES_PATH_1,TRAIN_FEATURES_PATH_2,TRAIN_FEATURES_PATH_3,TRAIN_FEATURES_PATH_4, TRAIN_FEATURES_PATH_5, TRAIN_FEATURES_PATH_6, TRAIN_FEATURES_PATH_7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set : train & test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was divided into training and testing sets for each class, with a split of 80% for training and 20% for testing, maintaining the same ratio for each class. However, the dataset exhibits an imbalance issue, with one class having a significantly larger number of samples compared to the other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/data_for_DL_augmented/dead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  dead  : train :  78  test :  4\n",
      "D:/data_for_DL_augmented/empty\n",
      "Class  empty  : train :  72  test :  3\n",
      "D:/data_for_DL_augmented/keep0\n",
      "Class  keep0  : train :  96  test :  5\n",
      "D:/data_for_DL_augmented/keep1\n",
      "Class  keep1  : train :  114  test :  5\n",
      "D:/data_for_DL_augmented/keep2\n",
      "Class  keep2  : train :  90  test :  4\n",
      "D:/data_for_DL_augmented/reseed0\n",
      "Class  reseed0  : train :  24  test :  2\n",
      "D:/data_for_DL_augmented/reseed1\n",
      "Class  reseed1  : train :  48  test :  2\n",
      "D:/data_for_DL_augmented/split\n",
      "Class  split  : train :  54  test :  3\n"
     ]
    }
   ],
   "source": [
    "labels = ['dead', 'empty', 'keep0', 'keep1', 'keep2', 'reseed0', 'reseed1', 'split']\n",
    "for i, path in enumerate(PATHES_LIST):\n",
    "    print(path)\n",
    "    print(\"Class \", labels[i], \" : train : \",len(os.listdir(os.path.join(path, \"train\"))),\" test : \",len(os.listdir(os.path.join(path, \"test\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_test` is a custom dataset class used for testing or evaluating a machine learning model on a specific set of images. Here's a brief explanation of why it's needed:\n",
    "\n",
    "1. **Path List**: The `path_list` parameter contains the list of paths to directories where the images are located. Each directory represents a different class or category.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for testing. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the test set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for testing. This ensures that the test dataset contains the desired proportion of samples from each class.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the test set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. The image tensor and its corresponding label are then returned as a tuple.\n",
    "\n",
    "By implementing this custom dataset class, you can easily load and process the test data in a standardized manner, making it convenient to evaluate the model's performance on a specific test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)  # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            class_size_test = round(class_size * (1 - ratio))\n",
    "            self.len += class_size_test\n",
    "            self.img_labels.extend([i] * class_size_test)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")][-class_size_test:]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255  # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise train data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `CustomImageDataset_train` is another custom dataset class specifically designed for training a machine learning model using augmented images. Here's a brief explanation of why it's needed, along with the rationale behind using data augmentation techniques:\n",
    "\n",
    "1. **Path List**: Similar to the previous class, the `path_list` parameter contains the list of paths to directories where the training images are located.\n",
    "\n",
    "2. **Transform**: The `transform` parameter represents the image transformations that need to be applied to each training image, such as resizing, normalization, or augmentation.\n",
    "\n",
    "3. **Ratio**: The `ratio` parameter determines the proportion of images that will be used for training. It allows you to specify the desired split between the training and testing datasets.\n",
    "\n",
    "4. **Initialization**: During initialization, the class calculates the size of each class based on the number of image files in the corresponding directory. It then determines the number of images to be included in the training set based on the specified ratio.\n",
    "\n",
    "5. **Data Organization**: The class organizes the image paths and their corresponding labels, keeping only the images that will be used for training.\n",
    "\n",
    "6. **Length and Indexing**: The `__len__` method returns the total number of images in the training set, while the `__getitem__` method allows indexing to retrieve a specific image and its label.\n",
    "\n",
    "7. **Image Processing**: Within `__getitem__`, the class reads the image from the file path, converts it to a floating-point tensor, and applies the specified transformations. Additionally, the class applies data augmentation techniques to the training images to enhance the diversity and generalizability of the dataset.\n",
    "\n",
    "8. **Augmentation Techniques**: The `augmentation` method implements several augmentation techniques commonly used for image data. These techniques include random cropping, random horizontal and vertical flipping, and rotation by a random angle between 0 and 45 degrees. By applying these techniques randomly to each training image, the dataset becomes more robust and less sensitive to variations in the input data.\n",
    "\n",
    "The purpose of using data augmentation is to artificially increase the size and variability of the training dataset. This helps prevent overfitting and improves the model's ability to generalize to new, unseen data. By introducing random transformations during training, the model becomes more resilient to variations in the test data and can better handle real-world scenarios.\n",
    "\n",
    "Overall, the `CustomImageDataset_train` class provides a convenient way to load, preprocess, and augment the training data, enabling effective training of machine learning models on a diverse and expanded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_train(Dataset):\n",
    "    def __init__(self, path_list, transform, ratio, model_name):\n",
    "        self.path_list = path_list\n",
    "        self.len = 0\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "        self.transform = transform\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name) # Initialize the AutoImageProcessor\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            class_size = len([f for f in sorted(os.listdir(img_dir)) if f.endswith(\".jpg\")])\n",
    "            class_size_train = class_size - round(class_size * (1 - ratio))\n",
    "            self.len += class_size_train\n",
    "            self.img_labels.extend([i] * class_size_train)\n",
    "            self.img_sort.extend(sorted([os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith(\".jpg\")])[:class_size_train])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float() / 255. # Loading the image and normalizing\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = self.augmentation(image) # Applying custom augmentation\n",
    "\n",
    "        # Process the image for the model using the AutoImageProcessor\n",
    "        image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label\n",
    "        \n",
    "    def augmentation(self, image):\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "\n",
    "        # Random rotation by 0, 90, 180, or 270 degrees\n",
    "        degree = random.choice([0, 90, 180, 270])\n",
    "        image = TF.rotate(image, degree)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data to train and test data with ratio 0.8 in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize the CustomImageDataset_test and CustomImageDataset_train, it is necessary to specify the desired transformations for each case, such as Normalize and CenterCrop.\n",
    "\n",
    "The following code demonstrates how to define the transformations:\n",
    "\n",
    "For test data transformation using CenterCrop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset_test(Dataset):\n",
    "    def __init__(self, path_list, transform=None, model_name=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "        self.processor = None\n",
    "        \n",
    "        # Load the processor if a model name is given\n",
    "        if model_name is not None:\n",
    "            self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "        self.img_labels = []\n",
    "        self.img_sort = []\n",
    "\n",
    "        for i, img_dir in enumerate(path_list):\n",
    "            # Load all images in the directory\n",
    "            img_files = sorted([f for f in os.listdir(img_dir) if f.endswith(\".jpg\")])\n",
    "            self.img_labels.extend([i] * len(img_files))\n",
    "            self.img_sort.extend([os.path.join(img_dir, f) for f in img_files])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_sort)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_sort[idx]\n",
    "        image = read_image(img_path).float()\n",
    "        label = self.img_labels[idx]\n",
    "\n",
    "        if image.shape[0] == 1:  # assuming (channel, height, width)\n",
    "            image = image.repeat(3, 1, 1)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image for the model if a processor is available\n",
    "        if self.processor:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path_list = [os.path.normpath(os.path.join(path,\"train\")) for path in PATHES_LIST]\n",
    "test_path_list = [os.path.normpath(os.path.join(path,\"test\")) for path in PATHES_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\data_for_DL_augmented\\\\dead\\\\train', 'D:\\\\data_for_DL_augmented\\\\empty\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep0\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep1\\\\train', 'D:\\\\data_for_DL_augmented\\\\keep2\\\\train', 'D:\\\\data_for_DL_augmented\\\\reseed0\\\\train', 'D:\\\\data_for_DL_augmented\\\\reseed1\\\\train', 'D:\\\\data_for_DL_augmented\\\\split\\\\train']\n"
     ]
    }
   ],
   "source": [
    "print(train_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = CustomImageDataset_test(path_list=test_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')\n",
    "\n",
    "train_dataset = CustomImageDataset_test(path_list=train_path_list, \n",
    "                                  transform=None, \n",
    "                                  model_name='google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 576\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset), len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"google/vit-base-patch16-224\"\n",
    "num_labels = len(labels)  # The number of unique labels/classes in your dataset\n",
    "\n",
    "# Load the configuration of the model\n",
    "config = ViTConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# Instantiate the model with the new configuration\n",
    "model = ViTForImageClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForImageClassificationWithAttention(ViTForImageClassification):\n",
    "    \"\"\"\n",
    "    This class extends ViTForImageClassification to return attention weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, pixel_values, output_attentions=True):\n",
    "        outputs = self.vit(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        attention_weights = outputs.attentions if output_attentions else None\n",
    "        return logits, attention_weights\n",
    "\n",
    "# Replace the original ViT model with the modified one that returns attention weights\n",
    "model = ViTForImageClassificationWithAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlaurent-gurtler\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\laure\\Documents\\organoids-project\\notebooks\\wandb\\run-20231107_010532-ep8z6r58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/ep8z6r58' target=\"_blank\">fast-night-7</a></strong> to <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/laurent-gurtler/organoid_classification' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/ep8z6r58' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification/runs/ep8z6r58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1, Loss: 2.0455248885684543, Accuracy: 25.0 %\n",
      "Epoch 1, Loss: 2.0455248885684543, F1 Score: 0.12643678160919541\n",
      "Epoch 2, Loss: 1.6081446607907612, Accuracy: 28.571428571428573 %\n",
      "Epoch 2, Loss: 1.6081446607907612, F1 Score: 0.19727011494252872\n",
      "Epoch 3, Loss: 1.5306927959124248, Accuracy: 39.285714285714285 %\n",
      "Epoch 3, Loss: 1.5306927959124248, F1 Score: 0.2333333333333333\n",
      "Epoch 4, Loss: 1.3942236834102206, Accuracy: 28.571428571428573 %\n",
      "Epoch 4, Loss: 1.3942236834102206, F1 Score: 0.2442476383265857\n",
      "Epoch 5, Loss: 1.3060064646932814, Accuracy: 35.714285714285715 %\n",
      "Epoch 5, Loss: 1.3060064646932814, F1 Score: 0.22489495798319328\n",
      "Epoch 6, Loss: 1.1970935066541035, Accuracy: 42.857142857142854 %\n",
      "Epoch 6, Loss: 1.1970935066541035, F1 Score: 0.2964285714285715\n",
      "Epoch 7, Loss: 1.1841022902064853, Accuracy: 42.857142857142854 %\n",
      "Epoch 7, Loss: 1.1841022902064853, F1 Score: 0.3112179487179487\n",
      "Epoch 8, Loss: 1.0501008364889357, Accuracy: 42.857142857142854 %\n",
      "Epoch 8, Loss: 1.0501008364889357, F1 Score: 0.3991228070175438\n",
      "Epoch 9, Loss: 0.9891356196668413, Accuracy: 35.714285714285715 %\n",
      "Epoch 9, Loss: 0.9891356196668413, F1 Score: 0.22181372549019607\n",
      "Epoch 10, Loss: 0.8927945362197028, Accuracy: 46.42857142857143 %\n",
      "Epoch 10, Loss: 0.8927945362197028, F1 Score: 0.473150623885918\n",
      "Epoch 11, Loss: 0.8080269197622935, Accuracy: 42.857142857142854 %\n",
      "Epoch 11, Loss: 0.8080269197622935, F1 Score: 0.4140037593984962\n",
      "Epoch 12, Loss: 0.6602880722946591, Accuracy: 57.142857142857146 %\n",
      "Epoch 12, Loss: 0.6602880722946591, F1 Score: 0.5880952380952381\n",
      "Epoch 13, Loss: 0.5678611199061075, Accuracy: 64.28571428571429 %\n",
      "Epoch 13, Loss: 0.5678611199061075, F1 Score: 0.6408730158730158\n",
      "Epoch 14, Loss: 0.5334593090746138, Accuracy: 60.714285714285715 %\n",
      "Epoch 14, Loss: 0.5334593090746138, F1 Score: 0.6218253968253968\n",
      "Epoch 15, Loss: 0.44135509265793693, Accuracy: 64.28571428571429 %\n",
      "Epoch 15, Loss: 0.44135509265793693, F1 Score: 0.6488324175824176\n",
      "Epoch 16, Loss: 0.4144020643499162, Accuracy: 64.28571428571429 %\n",
      "Epoch 16, Loss: 0.4144020643499162, F1 Score: 0.6308760683760684\n",
      "Epoch 17, Loss: 0.48096005535788006, Accuracy: 64.28571428571429 %\n",
      "Epoch 17, Loss: 0.48096005535788006, F1 Score: 0.6197649572649573\n",
      "Epoch 18, Loss: 0.4020610600709915, Accuracy: 64.28571428571429 %\n",
      "Epoch 18, Loss: 0.4020610600709915, F1 Score: 0.6797619047619048\n",
      "Epoch 19, Loss: 0.3135645062559181, Accuracy: 71.42857142857143 %\n",
      "Epoch 19, Loss: 0.3135645062559181, F1 Score: 0.7126581751581751\n",
      "Epoch 20, Loss: 0.2999861049983237, Accuracy: 60.714285714285715 %\n",
      "Epoch 20, Loss: 0.2999861049983237, F1 Score: 0.4916666666666666\n",
      "Epoch 21, Loss: 0.26352588190800613, Accuracy: 67.85714285714286 %\n",
      "Epoch 21, Loss: 0.26352588190800613, F1 Score: 0.7060876623376623\n",
      "Epoch 22, Loss: 0.2622673850920465, Accuracy: 67.85714285714286 %\n",
      "Epoch 22, Loss: 0.2622673850920465, F1 Score: 0.6934274059274059\n",
      "Epoch 23, Loss: 0.1681913271960285, Accuracy: 71.42857142857143 %\n",
      "Epoch 23, Loss: 0.1681913271960285, F1 Score: 0.7456730769230769\n",
      "Epoch 24, Loss: 0.1998828881316715, Accuracy: 78.57142857142857 %\n",
      "Epoch 24, Loss: 0.1998828881316715, F1 Score: 0.7968482905982905\n",
      "Epoch 25, Loss: 0.1811620183289051, Accuracy: 67.85714285714286 %\n",
      "Epoch 25, Loss: 0.1811620183289051, F1 Score: 0.6229166666666666\n",
      "Epoch 26, Loss: 0.12480911053717136, Accuracy: 75.0 %\n",
      "Epoch 26, Loss: 0.12480911053717136, F1 Score: 0.7921016483516484\n",
      "Epoch 27, Loss: 0.10812201050834523, Accuracy: 67.85714285714286 %\n",
      "Epoch 27, Loss: 0.10812201050834523, F1 Score: 0.63125\n",
      "Epoch 28, Loss: 0.11169260388447179, Accuracy: 71.42857142857143 %\n",
      "Epoch 28, Loss: 0.11169260388447179, F1 Score: 0.7041666666666667\n",
      "Epoch 29, Loss: 0.22818958717915747, Accuracy: 60.714285714285715 %\n",
      "Epoch 29, Loss: 0.22818958717915747, F1 Score: 0.6110479797979798\n",
      "Epoch 30, Loss: 0.1403854594876369, Accuracy: 71.42857142857143 %\n",
      "Epoch 30, Loss: 0.1403854594876369, F1 Score: 0.7250000000000001\n",
      "Epoch 31, Loss: 0.07782790509776936, Accuracy: 82.14285714285714 %\n",
      "Epoch 31, Loss: 0.07782790509776936, F1 Score: 0.8527777777777779\n",
      "Epoch 32, Loss: 0.04886388724359373, Accuracy: 71.42857142857143 %\n",
      "Epoch 32, Loss: 0.04886388724359373, F1 Score: 0.7357371794871794\n",
      "Epoch 33, Loss: 0.04678237526160148, Accuracy: 75.0 %\n",
      "Epoch 33, Loss: 0.04678237526160148, F1 Score: 0.7888278388278389\n",
      "Epoch 34, Loss: 0.13454492100410992, Accuracy: 64.28571428571429 %\n",
      "Epoch 34, Loss: 0.13454492100410992, F1 Score: 0.6646103896103897\n",
      "Epoch 35, Loss: 0.3132111372219192, Accuracy: 67.85714285714286 %\n",
      "Epoch 35, Loss: 0.3132111372219192, F1 Score: 0.5606740481740482\n",
      "Epoch 36, Loss: 0.3124719864378373, Accuracy: 57.142857142857146 %\n",
      "Epoch 36, Loss: 0.3124719864378373, F1 Score: 0.5652056277056277\n",
      "Epoch 37, Loss: 0.19768699631094933, Accuracy: 71.42857142857143 %\n",
      "Epoch 37, Loss: 0.19768699631094933, F1 Score: 0.7248397435897436\n",
      "Epoch 38, Loss: 0.243099771026108, Accuracy: 64.28571428571429 %\n",
      "Epoch 38, Loss: 0.243099771026108, F1 Score: 0.6547979797979798\n",
      "Epoch 39, Loss: 0.25448167531026733, Accuracy: 78.57142857142857 %\n",
      "Epoch 39, Loss: 0.25448167531026733, F1 Score: 0.8155753968253969\n",
      "Epoch 40, Loss: 0.1802262839757734, Accuracy: 82.14285714285714 %\n",
      "Epoch 40, Loss: 0.1802262839757734, F1 Score: 0.8524801587301587\n",
      "Epoch 41, Loss: 0.1255042797161473, Accuracy: 75.0 %\n",
      "Epoch 41, Loss: 0.1255042797161473, F1 Score: 0.762026862026862\n",
      "Epoch 42, Loss: 0.05220954813477066, Accuracy: 75.0 %\n",
      "Epoch 42, Loss: 0.05220954813477066, F1 Score: 0.7611111111111111\n",
      "Epoch 43, Loss: 0.031024793059461646, Accuracy: 82.14285714285714 %\n",
      "Epoch 43, Loss: 0.031024793059461646, F1 Score: 0.8182539682539682\n",
      "Epoch 44, Loss: 0.01226573533171581, Accuracy: 82.14285714285714 %\n",
      "Epoch 44, Loss: 0.01226573533171581, F1 Score: 0.8489448051948052\n",
      "Epoch 45, Loss: 0.009719606941669352, Accuracy: 78.57142857142857 %\n",
      "Epoch 45, Loss: 0.009719606941669352, F1 Score: 0.8001352813852813\n",
      "Epoch 46, Loss: 0.019990768341813236, Accuracy: 75.0 %\n",
      "Epoch 46, Loss: 0.019990768341813236, F1 Score: 0.7635149572649573\n",
      "Epoch 47, Loss: 0.058764222867062524, Accuracy: 75.0 %\n",
      "Epoch 47, Loss: 0.058764222867062524, F1 Score: 0.7501352813852813\n",
      "Epoch 48, Loss: 0.01861058758287173, Accuracy: 75.0 %\n",
      "Epoch 48, Loss: 0.01861058758287173, F1 Score: 0.7765873015873016\n",
      "Epoch 49, Loss: 0.012273361789993942, Accuracy: 75.0 %\n",
      "Epoch 49, Loss: 0.012273361789993942, F1 Score: 0.7818452380952381\n",
      "Epoch 50, Loss: 0.011156029753490455, Accuracy: 78.57142857142857 %\n",
      "Epoch 50, Loss: 0.011156029753490455, F1 Score: 0.7799963924963925\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de27ca0ec7d44539a6bbe3111e5c2550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▁▃▁▃▃▃▂▃▅▆▅▆▆▆▇▆▆▇█▇▆▇▅█▇▇▆▅▇▆█▇▇██▇▇▇█</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▂▂▂▃▃▄▂▄▅▆▆▆▆▆▇▇▆▇▇▇▆▇▆█▇▇▆▅▇▆█▇▇██▇▇▇▇</td></tr><tr><td>final_accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▆▆▆▅▅▅▄▄▃▃▃▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>78.57143</td></tr><tr><td>epoch</td><td>49</td></tr><tr><td>f1_score</td><td>0.78</td></tr><tr><td>final_accuracy</td><td>78.57143</td></tr><tr><td>final_f1_score</td><td>sklearn.metrics._cla...</td></tr><tr><td>loss</td><td>0.01116</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-night-7</strong> at: <a href='https://wandb.ai/laurent-gurtler/organoid_classification/runs/ep8z6r58' target=\"_blank\">https://wandb.ai/laurent-gurtler/organoid_classification/runs/ep8z6r58</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231107_010532-ep8z6r58\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize a new wandb run\n",
    "wandb.init(project=\"organoid_classification\", entity=\"laurent-gurtler\")\n",
    "\n",
    "# Configurations (hyperparameters and model architecture)\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "num_epochs = 50  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Logging the training loss\n",
    "    wandb.log({\"epoch\": epoch, \"train_loss\": running_loss/len(train_loader)})\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    validation_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, attention_weights = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # Move the labels and predictions to CPU for sklearn metrics\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            last_layer_attention = attention_weights[-1]\n",
    "\n",
    "        # Now you might want to average across the heads, and ignore the class token:\n",
    "        # Note: last_layer_attention[:, :, 0, 1:] has shape [32, num_heads, num_patches]\n",
    "        attention_map = last_layer_attention.mean(dim=1)[:, 0, 1:].detach().cpu()\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    # Logging the validation loss, accuracy, and F1 score\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"validation_loss\": validation_loss/len(test_loader),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Validation Loss: {validation_loss/len(test_loader)}, Accuracy: {accuracy} %, F1 Score: {f1}\")\n",
    "\n",
    "# Final log\n",
    "wandb.log({\n",
    "    \"final_accuracy\": accuracy,\n",
    "    \"final_f1_score\": f1\n",
    "})\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Close the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organoids-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
